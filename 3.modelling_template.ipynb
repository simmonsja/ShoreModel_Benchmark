{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "208f4655",
   "metadata": {},
   "source": [
    "# Simple regression models - ShoreShop 2.0\n",
    "\n",
    "Some simple regression models from old code. These are a nice baseline to explore the data, and get a feel for the process and alongshore behaviour. \n",
    "The code I have has the models estimate parameters for dx and then model y as:\n",
    "\n",
    "`y = jnp.cumsum(f(x)) + e`\n",
    "\n",
    "Things to deal with:\n",
    "\n",
    "- check estimate y directly for the simplest models before building on dx style models\n",
    "- autocorrelation in residuals (though low priority as not fussed about accuracy of uncertainty estimates)\n",
    "\n",
    "\n",
    "- Check the reason for the years that spike very high and low\n",
    "  - this appears to be at multiple profiles so I am fairly sure it it real, but need to correlate to certain events\n",
    "  - seems to be some form of rotation or similar as the profile react together\n",
    "- Can I use alongshore information in any way - like that was the shoreline predicted at the adjacent profiles last month.\n",
    "  - spatial lagged autoregression\n",
    "- NUTS appears the way and not SVI\n",
    "- try simple onestep ahead vs autoregression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c858186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# magic\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f998451",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from functions.data_load import load_modelling_data, tabularise_raw_data\n",
    "\n",
    "from functions.inference.utils import (\n",
    "    calc_skill,\n",
    "    print_skill\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11e7ffb",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd6404c",
   "metadata": {},
   "source": [
    "Load the data using `load_modelling_data()` that returns a dictionary containing the following pandas DataFrames mirroring `1.data_explore.ipynb` (with indices as datetime objects where possible):\n",
    "\n",
    "- `df_tran`: Transect data.\n",
    "- `df_gt`: Groundtruth shoreline positions.\n",
    "- `df_targ_short`: Short-term target shoreline positions.\n",
    "- `df_targ_medium`: Medium-term target shoreline positions.\n",
    "- `df_obs`: Observed shoreline positions.\n",
    "- `dfs_wave`: Wave data for different parameters ('Hs', 'Tp', 'Dir').\n",
    "- `df_SLR_obs`: Observed sea level rise data.\n",
    "- `df_SLR_proj`: Projected sea level rise data.\n",
    "\n",
    "Provided information:\n",
    "A sequence of shore-normal transects were defined from North to South.\\\n",
    "The distance between transects is 100 m alongshore\\\n",
    "The coordinates of transects were intentionally shifted.\n",
    "\n",
    "Shoreline positions were retrieved from Landsat 5, 7, 8 and 9 satellite images with [CoastSat toolbox](https://github.com/kvos/CoastSat/tree/master).\\\n",
    "All the shorelines have been corrected to reflect the instaneous position at Mean Sea Level\\\n",
    "Only shorelines from 1987 to 2018 are avaiable for model training/calibration in this site.\\\n",
    "Along each transect, shoreline positions were provided as the distance to the **landward** end of the transect.\n",
    "\n",
    "Offshore wave data is from [ERA5](https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels?tab=overview) wave spectra data.\\\n",
    "SWAN model was used to transfer offshore wave into nearshore with [Binwave](https://www.sciencedirect.com/science/article/pii/S1463500324000337) approach developed by Dr Laura Cagigal.\\\n",
    "Along each transect, the wave data was extracted at 10m contour "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05b5d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = '.' # where are you in relation to the root directory\n",
    "raw_data = load_modelling_data(basedir=basedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53cc54f",
   "metadata": {},
   "source": [
    "## Optional preprocessing\n",
    "Here is provided a template for optional preprocessing steps that can be used to tabularise the data for modelling. Adopted the frequency of the wave data, but this can of course be adjusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff44b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional pre-processing\n",
    "tabular_data = tabularise_raw_data(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e55329",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(tabular_data['df_obs'], hue='Transect', x='date', y='shoreline')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067cf414",
   "metadata": {},
   "source": [
    "### Quick visualisation\n",
    "Some quick visualisation of the data to show the format\n",
    "\n",
    "We have daily data with intermittent shoreline position at 9 transects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61af0f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tabular_data.keys())\n",
    "print(tabular_data['df_obs']['Transect'].unique())\n",
    "tabular_data['df_obs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355b677a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairplot the data\n",
    "trans_id = 'Transect1'\n",
    "sns.pairplot(\n",
    "    tabular_data['df_obs'].query('Transect == @trans_id').drop(columns=['Transect','date'])\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d5dba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the seasonal averages\n",
    "trans_id = 'Transect1'\n",
    "plot_data = tabular_data['df_obs'].query('Transect == @trans_id').drop(columns=['Transect']).set_index('date').resample('1ME').mean()\n",
    "plot_data = plot_data.assign(month=plot_data.index.month)\n",
    "# demean and stanardise the variables\n",
    "for _ in [_ for _ in plot_data.columns if not _ in ['month']]:\n",
    "    plot_data[_] = (plot_data[_] - plot_data[_].mean())/plot_data[_].std()\n",
    "sns.boxplot(data=plot_data.melt(id_vars='month'),x='month',y='value',hue='variable',boxprops=dict(alpha=.5))\n",
    "plt.legend(loc=6,bbox_to_anchor=(1.025,0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2886a05",
   "metadata": {},
   "source": [
    "### Prepare the data\n",
    "Initially I will resample to monthly to avoid so many NaNs and other unpleasantries. Then we will cheekily gap fill the data, this will be a big exercise in missing data and the choice around how this is handled needs to be considered carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0f1f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate how many non NaN values at each transect\n",
    "print(tabular_data['df_obs'].groupby('Transect').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9898c2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split per Transect then average to monthly values with mean and peak for Hs and Tp then recombine into a reasonable dataframe\n",
    "resampled_data = tabular_data['df_obs'].copy()\n",
    "resampled_data = pd.concat(\n",
    "    [\n",
    "        resampled_data.query('Transect == @trans_id').drop(columns=['Transect']).set_index('date').resample('MS').agg({'Hs':['mean','max'],'Tp':['mean','max'],'Dir':['mean'],'shoreline':['mean']}).reset_index().assign(Transect=trans_id) for trans_id in resampled_data['Transect'].unique()\n",
    "    ], axis=0\n",
    ")\n",
    "# combine the column names to make one level\n",
    "resampled_data.columns = [\n",
    "    '_'.join(col).strip() if '' != col[1] else col[0] for col in resampled_data.columns.values]\n",
    "resampled_data = resampled_data.rename(columns={'shoreline_mean':'shoreline'})\n",
    "# now add month predictor\n",
    "resampled_data = resampled_data.assign(month=resampled_data['date'].dt.month-1)\n",
    "resampled_data = resampled_data.reset_index(drop=True)\n",
    "# get the shoreline position at t-1 but only for the same transect\n",
    "resampled_data = resampled_data.assign(shoreline_tminus1=resampled_data.groupby('Transect')['shoreline'].shift(1))\n",
    "# Keep only Transect6\n",
    "resampled_data = resampled_data.query('Transect == \"Transect7\"').reset_index(drop=True)\n",
    "resampled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0118e52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vis of transect data\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "ax1 = fig.add_subplot(211)\n",
    "ax2 = fig.add_subplot(212)\n",
    "sns.lineplot(data=tabular_data['df_obs'],y='shoreline',x='date', hue='Transect', ax=ax1, alpha=0.25)\n",
    "sns.lineplot(data=tabular_data['df_obs'],y='shoreline',x='date', ax=ax1, color = 'k', errorbar=None)\n",
    "\n",
    "sns.lineplot(data=resampled_data,y='shoreline', x='date', hue='Transect', ax=ax2, alpha=0.25)\n",
    "sns.lineplot(data=resampled_data,y='shoreline', x='date', ax=ax2, color = 'k', errorbar=None)\n",
    "sns.lineplot(data=resampled_data.query('Transect == \"Transect1\"'),y='shoreline', x='date', color = 'C0', ax=ax2, alpha=0.75)\n",
    "sns.lineplot(data=resampled_data.query('Transect == \"Transect9\"'),y='shoreline', x='date', color = 'C3', ax=ax2, alpha=0.75)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9164797c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training 1999-2016 and testing 2017-2019\n",
    "train_data = resampled_data.query('date < \"2017-01-01\"').reset_index(drop=True)\n",
    "test_data = resampled_data.query('date >= \"2017-01-01\"').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95903399",
   "metadata": {},
   "source": [
    "# Create the modelling copies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a387bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_clean = train_data.dropna().copy()\n",
    "test_data_clean = test_data.dropna().copy()\n",
    "train_data_out = train_data_clean.copy()\n",
    "test_data_out = test_data_clean.copy()\n",
    "\n",
    "display(train_data_clean)\n",
    "display(test_data_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe26808",
   "metadata": {},
   "source": [
    "## Modelling attempt\n",
    "Here you can construct your model and make some predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42343fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming train_data and test_data are already defined DataFrames\n",
    "# Define the feature matrix X and target vector y\n",
    "X_train = train_data_clean.drop(columns=['date', 'shoreline', 'Transect'])\n",
    "y_train = train_data_clean['shoreline']\n",
    "X_test = test_data_clean.drop(columns=['date', 'shoreline', 'Transect'])\n",
    "\n",
    "# Initialize the scalers\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "# Scale the feature matrix and target vector\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1))\n",
    "\n",
    "# Create and train the regression model\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "# Prepare the test data\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_scaled = model.predict(X_test_scaled)\n",
    "# Rescale the predictions back to the original scale\n",
    "predictions = scaler_y.inverse_transform(predictions_scaled.reshape(-1, 1))\n",
    "\n",
    "# If you want to add the predictions to the test_data DataFrame\n",
    "test_data_out['predicted_shoreline'] = predictions\n",
    "\n",
    "# Make predictions on the training data\n",
    "predictions_scaled_train = model.predict(X_train_scaled)\n",
    "# Rescale the predictions back to the original scale\n",
    "predictions_train = scaler_y.inverse_transform(predictions_scaled_train.reshape(-1, 1))\n",
    "train_data_out['predicted_shoreline'] = predictions_train\n",
    "\n",
    "# Display the first few rows of the test_data with predictions\n",
    "print(train_data_out.head())\n",
    "print(test_data_out.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a603236",
   "metadata": {},
   "source": [
    "# Plot predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21627ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of the predicted vs. observed shoreline positions\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "sns.scatterplot(data=test_data_out, x='shoreline', y='predicted_shoreline', ax=ax1)\n",
    "sns.scatterplot(data=train_data_out, x='shoreline', y='predicted_shoreline', ax=ax2)\n",
    "\n",
    "ax1.set_title('Test data')\n",
    "ax2.set_title('Train data')\n",
    "\n",
    "ax1.set_xlabel('Observed shoreline position')\n",
    "ax1.set_ylabel('Predicted shoreline position')\n",
    "ax2.set_xlabel('Observed shoreline position')\n",
    "ax2.set_ylabel('Predicted shoreline position')\n",
    "\n",
    "plt.subplots_adjust(wspace=0.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9039e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot timeseries of observed and predicted shoreline positions\n",
    "plt.figure(figsize=(8, 3))\n",
    "sns.lineplot(data=train_data, x='date', y='shoreline', label='Observed shoreline position')\n",
    "sns.lineplot(data=train_data_out, x='date', y='predicted_shoreline', label='Predicted shoreline position')\n",
    "plt.ylabel('Shoreline position')\n",
    "plt.title('Observed and predicted shoreline positions - Training data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152e3ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot timeseries of observed and predicted shoreline positions\n",
    "plt.figure(figsize=(8, 3))\n",
    "sns.lineplot(data=test_data, x='date', y='shoreline', label='Observed shoreline position')\n",
    "sns.lineplot(data=test_data_out, x='date', y='predicted_shoreline', label='Predicted shoreline position')\n",
    "plt.ylabel('Shoreline position')\n",
    "plt.title('Observed and predicted shoreline positions - Test data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93524738",
   "metadata": {},
   "source": [
    "### Compare model skill\n",
    "Compare the skill of each model by RMSE, R2 (of dShl), BSS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c6f1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# report the RMSE, BSS and R2\n",
    "print('#'*80)\n",
    "print('Train - one step')\n",
    "rmse, r2, bss, r = calc_skill(\n",
    "    curr_obs=train_data_out['shoreline'],\n",
    "    prev_obs=train_data_out['shoreline_tminus1'],\n",
    "    mean_mu=train_data_out['predicted_shoreline'].values\n",
    ")\n",
    "print('BSS: {:.2f} | RMSE: {:.2f} | R2: {:.2f} | r: {:.2f}'.format(bss,rmse,r2,r))\n",
    "print('#'*80)\n",
    "print('Test - one step')\n",
    "rmse, r2, bss, r = calc_skill(\n",
    "    curr_obs=test_data_out['shoreline'],\n",
    "    prev_obs=test_data_out['shoreline_tminus1'],\n",
    "    mean_mu=test_data_out['predicted_shoreline'].values\n",
    ")\n",
    "print('BSS: {:.2f} | RMSE: {:.2f} | R2: {:.2f} | r: {:.2f}'.format(bss,rmse,r2,r))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
